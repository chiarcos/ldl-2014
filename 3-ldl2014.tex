\section{LDL-2014: The 3rd Workshop on Linked Data in Linguistics}

For the 3rd edition of the workshop on Linked Data in Linguistics, we invited contributions discussing the application of the Linked Open Data paradigm to linguistic data in various fields of linguistics, natural language processing, knowledge management and information technology in order to to present and discuss \emph{principles}, \emph{case studies}, and \emph{best practices} for representing, publishing and linking mono- and multilingual linguistic and knowledge data collections, including corpora, grammars, dictionaries, wordnets, translation memories, domain specific ontologies etc. 

In this regard, the Linked Data paradigm might provide an important step towards making linguistic data: i) easily and uniformly queryable, ii) interoperable and iii) sharable over the Web using open standards such as the HTTP protocol and the RDF data model. The adaptation of some processes and best practices to \textbf{multilingual linguistic resources and knowledge bases} acquires special relevance in this context. Some processes may need to be modified to accommodate the publication of resources that contain information in several languages. Also the linking process between linguistic resources in different languages poses important research questions, as well as the development and application of freely available knowledge bases and crowdsourcing to compensate the lack of publicly accessible language resources for various languages.

Further, LDL-2014 provides a forum for researchers on natural language processing and semantic web technologies to present case studies and best practices on the exploitation of linguistic resources exposed on the Web for \textbf{Natural Language Processing} applications, or other content-centered applications such as content analytics, knowledge extraction, etc. The availability of massive linked open knowledge resources raises the question how such data can be suitably employed to facilitate different NLP tasks and research questions. Following the tradition of earlier LDL workshops, we encouraged contributions to the Linguistic Linked Open Data (LLOD) cloud and research on this basis. In particular, this pertains to contributions that demonstrate an added value resulting from the combination of linked datasets and ontologies as a source for semantic information with linguistic resources published according to as linked data principles. Another important question to be addressed in the workshop is how Natural Language Processing techniques can be employed to further facilitate the growth and enrichment of linguistic resources on the Web.

The call for papers emphasized the following topics:

\begin{enumerate}
\item \textbf{Use cases} %and project proposals 
for creating or publishing linked linguistic data collections
\item \textbf{Modelling} linguistic data and metadata with OWL and/or RDF
\item \textbf{Ontologies} for linguistic data and metadata collections as well as for cross-lingual retrieval
\item Description of \textbf{data sets} following Linked Data principles
\item \textbf{Applications of such data}, other ontologies or linked data from any subdiscipline of linguistics % (may include work in progress or project descriptions)
\item \textbf{NLP\&LLOD}: Application and applicability of (Linguistic) Linked Open Data in NLP / NLP contributions to (Linguistic) Linked Open Data
\item Challenges of \textbf{multilinguality} and \textbf{collaboratively constructed open resources} % and the use of LOD %and collaboratively constructed open resources 
for knowledge extraction, machine translation and other NLP tasks.
\item \textbf{Legal and social aspects} of (L)LOD
\item \textbf{Best practices} for the publication and linking of multilingual knowledge resources
\end{enumerate}

\noindent 
Along with regular workshop submissions, we invited contributions to the associated data challenge (see below) for data sets together with data set descriptions.
In total, we received 19 submissions in response to our calls, including 5 data set descriptions for the associated data challenge. Regular submissions were reviewed by at least 3 
% actually 4, IMHO
members of the program committee. 
On this basis, we accepted 6 submissions as full papers and 4 as short papers.

The 10 accepted papers address a wide range of problems in the area of NLP and (Linguistic) Linked Open Data, pertaining to modeling, representation, analysis and publishing of various 
data or metadata.

Taken together, the contributions cover a vast and heterogeneous field, they involve different types of linguistic resources, such as machine-readable lexicons, etymological and diachronic databases, web, movies, and grammar terminology, but also address issues of localization and multilinguality. Our tentative classification, that we apply both to the proceedings and the remainder of this section, is a compromise between a classification on grounds of resource types and prospective applications:

A particularly popular branch of research is concerned with \textbf{modeling lexical-semantic resources} using RDF-based vocabularies and lexicon-to-ontology mappings, most noteably \emph{lemon}. 
This group of submissions partially overlaps with a surprisingly large number of papers concerned with the modeling of multilingual resources in more academic fields of linguistics, namely \textbf{cross-linguistic studies} in linguistic typology and comparative linguistics.
A third group of papers involves different conceptions of \textbf{metadata}, i.e., terminology for linguistic categories and language resources, but also annotations to multimedial content.
Finally, we sketch the contributions to the data set challenge, all of which were concerned with lexical-semantic resources.

\subsection{Modelling Lexical-Semantic Resources with \emph{lemon}}

	In their paper \textbf{Attaching translations to proper lexical senses in DBnary}, 
	Andon Tchechmedjiev, Gilles Sérasset, Jérôme Goulian and Didier Schwab 
	present	the current status of the DBnary project: DBnary aims at extracting linked open data from Wiktionaries in various languages, for which 
	the authors present a similarity technique for disambiguation of linked translations. 

	John Philip McCrae, Christiane Fellbaum and Philipp Cimiano describe their approach on \textbf{Publishing and linking WordNet using \emph{lemon} and RDF} where they
	propose a strategy for publishing the Princeton WordNet as linked data through an open model. 
	The advantage of this approach is that it provides linking also to the resources which have been already integrated into WordNet. 	

	The paper \textbf{Releasing genre keywords of Russian movie descriptions as Linked Open Data: An experience report} by Andrey Kutuzov and Maxim Ionov 
	describes efforts on publishing genre-classified movie keywords as LOD using the \emph{lemon} model. 
	The resource is also linked to Russian component of the Wiktionary RDF dump created by the DBpedia team.\footnote{
		\url{http://dbpedia.org/Wiktionary}
	}

\subsection{Cross-linguistic Studies: Applications in Comparative Linguistics and Typology}

Although most of the following papers also involve lexical resources, they are special in their domain of application, i.e., the study of cross-linguistic and/or diachronic relationships in linguistics.

	In \textbf{Linking etymological databases. A case study in Germanic}, Christian Chiarcos and Maria Sukhareva
	describe the modeling of etymological dictionaries of various Germanic languages in a machine-readable way as Linguistic Linked Open Data. 
	The authors adopted \emph{lemon}, and identified several problematic aspects in its application to this kind of data.
	The work is challenging, since it handles different language stages, but the current model represents a solid basis to discuss possible adjustments of both \emph{lemon} and the authors' approach in order to develop a \emph{lemon}-conformant representation that meets the requirements of diachronic data.
		
	More focusing on semantic shift than etymological (phonological) continuity, but operating in a similar setting, 
	Fahad Khan, Federico Boschetti and Francesca Frontini describe an approach on \textbf{Using \emph{lemon} to model lexical semantic shift in diachronic lexical resources}. 
	They propose \emph{lemonDIA}, an ontology-based extension of the \emph{lemon} model for representing lexical semantic change in temporal context that formalizes notions of perdurance and temporal anchoring of lexical senses. 

	Coming from the slightly different angle of cross-linguistic language comparison in linguistic typology, the paper 
	\textbf{Typology with graphs and matrices} by Steven Moran and Michael Cysouw describes how to extract information from LLOD representations of different typological data sets, and how to transform and operate with the extracted information in order to determine associations between syntactic and phonological features.

	Robert Forkel introduces \textbf{The Cross-Linguistic Linked Data project},
	an ongoing initiative and its infrastructure aiming towards establishing a platform 
	for interoperability among various language resources assembled in typological research. 
	The important role of Linguistic Linked Open Data has long been recognized as publishing strategy for 
	typological datasets \citep{ldl2012}, but here, a unified 
	publication platform is described which may have a considerable effect on the typological publicating practice.
	
\subsection{Metadata}

	As used here, metadata refers to information provided \emph{about} another resource, including language resources, linguistic terminology and multimedia contents.
	
	\textbf{From CLARIN Component Metadata to Linked Open Data} by 
	Matej Durco and Menzo Windhouwer describes the conversion from CMDI resource descriptions to LOD. 
	As a result, the RDF metadata can be accessed with standard query languages using SPARQL endpoints. 

	In \textbf{Towards a Linked Open Data Rrepresentation of a grammar terms index},
	Daniel Jettka, Karim Kuropka, Cristina Vertan and Heike Zinsmeister introduce onoing work on creating a Linked Open Data 
	representation of German grammatical terminology, an effort which nicely complements established efforts to create repositories for linguistic terminology used in language documentation, NLP and the development of machine-readable lexicons. Given the great amount of language-specific terminology, the proposed strategy is also applicable to other languages and their linking may eventually improve the multilingual coverage of linguistic terminology repositories.

	A different kind of metadata is subject to \textbf{A brief survey of multimedia annotation localization on the web of Linked Data} by 
	Gary Lefman, David Lewis and Felix Sasaki. The authors focus on the localization of multimedia ontologies and Linked Data frameworks for Flickr data. 
	In this respect, Linguistic Linked Open Data may serve as a mediator between multimedia annotation in social media and the Web of Linked Data. 

\subsection{Data Challenge}

The workshop was associated with an open challenge for the creation of datasets for linguistics according to linked data principles.
Unlike the preceding Monnet challenge\footnote{
	\url{http://sabre2012.infai.org/mlode/monnet-challenge}
} 
that was organized by the W3C OntoLex community at MLODE-2012, the LDL-2014 was not restricted to the application of the \emph{lemon} format. 
Nevertheless, all submissions were, indeed, lexical-semantic resources.
 
This challenge required submissions of new or substantially updated linked datasets and was evaluated by reviewers on technical grounds. 
The following criteria were applied: 

\begin{enumerate}[1.]
\item \emph{Availability}, i.e. (a) whether the resource uses Linked Data and RDF, (b) whether it is hosted on a publicly accessible server and is available both during the period of the evaluation and beyond, and (c) whether it uses an open license.
\item \emph{Quality}, i.e. (a) whether the resource represents useful linguistically or NLP-relevant information, (b) whether it reuses relevant standards and models, and (c) wheter it contains complex, non-trivial information (e.g., multiple levels of annotation, manually validated analyses).
\item \emph{Linking}, i.e., (a) wheter the resource contains links to external resources, and (b) whether it reuses existing properties and categories.
\item \emph{Impact/usefulness} of the resource, i.e., (a) whether it is relevant and likely to be reused by many researchers in NLP and beyond, and (b) whether it uses linked data to improve the quality of and access to the resource.
\item \emph{Originality}, i.e., (a) whether the data set represents a type of resource or a community currently underrepresented in (L)LOD cloud activities, or (b) whether the approach facilitates novel and unforeseen applications or use cases (as described by the authors) enabled through Linked Data technology.
\end{enumerate}

\noindent	
This year there were five accepted submissions to the challenge. 
Every challenge committee member provided a ranking of these resources, and the average rank was taken as decisive criterion.
In this process, we chose two joint winners and one highly commended paper. 

The winners 
were \textbf{DBnary: Wiktionary as Linked Data for 12 Language Editions with Enhanced 
Translation Relations} by Gilles Sérraset and Andon Tchechmedjiev and \textbf{Linked-data 
based domain-specific sentiment lexicons} by Gabriela Vulcu, Raul Lario Monje, 
Mario Munoz, Paul Buitelaar and Carlos A. Iglesias, describing the EuroSentiment 
lexicon. 
An outstanding characteristic of the DBnary data is its high degree of maturity (quality, usefulness, linking, availability). 
The EuroSentiment dataset is specifically praised for its originality and quality, as it represents the \emph{only} manually corrected sentiment lexicon currently available as Linguistic Linked Open Data.

\textbf{Sérraset and Tchechmedjiev} describe the extraction of multilingual data from Wiktionary 
based on 12 language editions of Wiktionary, and as such represents a large and 
important lexical resource that should have application in many linguistic areas. 
\textbf{Vulcu et al.} describe the creation of a lexicon for the EuroSentiment project, which 
tackles the important field of sentiment analysis through the use of sophisticated 
linguistic processing. The resource described extends the \textit{lemon} model 
with the MARL vocabulary to provide a lexicon that is unique in the field of sentiment 
analysis due to its linguistic sophistication. 

Beyond this, we highly commend the work presented in \textbf{A multilingual semantic network as linked data: Lemon-BabelNet} by
Maud Ehrmann, Francesco Cecconi, Daniele Vannelle, John P. McCrae, Philipp Cimiano 
and Roberto Navigli, which describes the expression of BabelNet using the \textit{lemon} 
vocabulary. BabelNet is one of the largest lexical resources created to date and 
its linked data version at over 1 billion triples will be one of the largest resources 
in the LLOD cloud. As such, the clear usefulness of the resource as a target for 
linking and also the use of the widely-used \textit{lemon} model make this conversion 
a highly valuable resource for the community as noted by the reviewers. 

Finally, 
we will note that our two runner-up participants \textbf{PDEV-LEMON: A linked data implementation 
of the pattern dictionary of English verbs based on the \textit{lemon} model} 
by  Ismail El Maarouf, Jane Bradbury and Patrick Hanks, and \textbf{Linked Hypernyms Dataset - Generation Framework 
and Use Cases} by Tomáš Kliegr, Vaclav Zeman and Milan Dojchinovski were also well received as resources that 
continue to grow the linguistic linked open data cloud and are likely to find applications 
for a number of works in linguistics and natural language processing.

\subsection{Invited Talks}

In addition to regular papers and dataset descriptions, LDL-2014 features two invited speakers, Piek Vossen, VU Amsterdam, and Gerard de Melo, Tsinghua University.

\smallskip

\textbf{Piek Th.J.M. Vossen} is a Professor of computational lexicology at the Vrije Universiteit Amsterdam, The Netherlands. He graduated from the University of Amsterdam in Dutch and general linguistics, where he obtained a PhD in computational lexicology in 1995, and is probably most well-known for being founder and president of the Global WordNet Association.

In his talk, he will describe and elaborate on the application of \textbf{The Collaborative Inter-Lingual-Index for harmonizing WordNets}. 
The Inter-Lingual-Index, originally developed in the context of EuroWordNet, provides a set of common reference points through which WordNets can be linked with each other across different languages and thereby establishes a semantic layer for the interpretation of text in a multilingual setting. Although devised before the advent of modern Linked Data technology, the applications developed on this basis are inspiring for applications of Linguistic Linked Open Data and we are therefore very happy to welcome Piek for discussions and exchange of ideas.


\smallskip

\textbf{Gerard de Melo} is an Assistant Professor at Tsinghua
 University, where he is heading the Web Mining and Language Technology
 group. Previously, he was a post-doctoral researcher at the the ICSI AI group of the UC Berkeley, and a doctoral candidate at the Max Planck
 Institute for Informatics.

In his talk, Gerard de Melo will describe the transition \textbf{From Linked Data to Tightly Integrated Data}. 
He argues that the true potential of Linked Data can only be appreciated when extensive cross-linkage and integration leads to an even higher degree of interconnectedness. Gerard compares different approaches on integration into unified, coherent knowledge bases and develops ideas on how to address some remaining challenges that are currently
 impeding a more widespread adoption of Linked Data. 
